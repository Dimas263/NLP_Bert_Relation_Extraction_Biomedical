2022-06-01 20:53:36,141 - INFO - main.py - <module> - 210 - {'Negative': 0, 'Cause_of_disease': 1, 'Treatment_of_disease': 2, 'Association': 3}
2022-06-01 20:53:36,141 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 157 - example_text : Halothane is known to oppose <e1start> digitalis <e1end>-induced <e2start> ventricular arrhythmias <e2end>. 
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 159 - example_id_tags : [29, 56, 65, 106]
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 157 - example_text : Both cases proved to be <e1start> cotton <e1end>-material-induced <e2start> granulomas <e2end>. 
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 159 - example_id_tags : [24, 48, 66, 94]
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 157 - example_text : The evidence for <e1start> soybean <e1end> products as <e2start> cancer <e2end> preventive agents.  
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 158 - example_id_label : Treatment_of_disease
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 159 - example_id_tags : [17, 42, 55, 79]
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 157 - example_text : [Mortality trends in <e2start> cancer <e2end> attributable to <e1start> tobacco <e1end> in Mexico].  
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 159 - example_id_tags : [62, 87, 21, 45]
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 157 - example_text : <e1start> Areca <e1end> nut chewing has a significant association with <e2start> systemic inflammation <e2end>.
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 158 - example_id_label : Association
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 159 - example_id_tags : [0, 23, 71, 110]
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 157 - example_text : <e2start> major depression <e2end> (MD) and regular <e1start> tobacco <e1end> use (RU) or nicotine dependence (ND).
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 158 - example_id_label : Association
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 159 - example_id_tags : [52, 77, 0, 34]
2022-06-01 20:53:36,156 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,203 - INFO - preprocess.py - convert_examples_to_features - 120 - Convert 46 examples to features
2022-06-01 20:53:36,203 - INFO - preprocess.py - convert_bert_example - 95 - *** train_example-0 ***
2022-06-01 20:53:36,203 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] H a l o t h a n e [UNK] i s [UNK] k n o w n [UNK] t o [UNK] o p p o s e [UNK] < e 1 s t a r t > [UNK] d i g i t a l i s [UNK] < e 1 e n d > - i n d u c e d [UNK] < e 2 s t a r t > [UNK] v e n t r i c u l a r [UNK] a r r h y t h m i a s [UNK] < e 2 e n d >. [UNK] [SEP]
2022-06-01 20:53:36,203 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 145, 170, 181, 184, 189, 177, 170, 183, 174, 100, 178, 188, 100, 180, 183, 184, 192, 183, 100, 189, 184, 100, 184, 185, 185, 184, 188, 174, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 173, 178, 176, 178, 189, 170, 181, 178, 188, 100, 133, 174, 122, 174, 183, 173, 135, 118, 178, 183, 173, 190, 172, 174, 173, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 191, 174, 183, 189, 187, 178, 172, 190, 181, 170, 187, 100, 170, 187, 187, 177, 194, 189, 177, 182, 178, 170, 188, 100, 133, 174, 123, 174, 183, 173, 135, 119, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,203 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,203 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,203 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,203 - INFO - preprocess.py - convert_bert_example - 101 - ids：[30, 57, 66, 107]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 95 - *** train_example-1 ***
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] B o t h [UNK] c a s e s [UNK] p r o v e d [UNK] t o [UNK] b e [UNK] < e 1 s t a r t > [UNK] c o t t o n [UNK] < e 1 e n d > - m a t e r i a l - i n d u c e d [UNK] < e 2 s t a r t > [UNK] g r a n u l o m a s [UNK] < e 2 e n d >. [UNK] [SEP]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 139, 184, 189, 177, 100, 172, 170, 188, 174, 188, 100, 185, 187, 184, 191, 174, 173, 100, 189, 184, 100, 171, 174, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 172, 184, 189, 189, 184, 183, 100, 133, 174, 122, 174, 183, 173, 135, 118, 182, 170, 189, 174, 187, 178, 170, 181, 118, 178, 183, 173, 190, 172, 174, 173, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 176, 187, 170, 183, 190, 181, 184, 182, 170, 188, 100, 133, 174, 123, 174, 183, 173, 135, 119, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 101 - ids：[25, 49, 67, 95]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 95 - *** train_example-2 ***
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] T h e [UNK] e v i d e n c e [UNK] f o r [UNK] < e 1 s t a r t > [UNK] s o y b e a n [UNK] < e 1 e n d > [UNK] p r o d u c t s [UNK] a s [UNK] < e 2 s t a r t > [UNK] c a n c e r [UNK] < e 2 e n d > [UNK] p r e v e n t i v e [UNK] a g e n t s. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 157, 177, 174, 100, 174, 191, 178, 173, 174, 183, 172, 174, 100, 175, 184, 187, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 188, 184, 194, 171, 174, 170, 183, 100, 133, 174, 122, 174, 183, 173, 135, 100, 185, 187, 184, 173, 190, 172, 189, 188, 100, 170, 188, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 172, 170, 183, 172, 174, 187, 100, 133, 174, 123, 174, 183, 173, 135, 100, 185, 187, 174, 191, 174, 183, 189, 178, 191, 174, 100, 170, 176, 174, 183, 189, 188, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 100 - labels: 2
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 101 - ids：[18, 43, 56, 80]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 95 - *** train_example-3 ***
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] [ M o r t a l i t y [UNK] t r e n d s [UNK] i n [UNK] < e 2 s t a r t > [UNK] c a n c e r [UNK] < e 2 e n d > [UNK] a t t r i b u t a b l e [UNK] t o [UNK] < e 1 s t a r t > [UNK] t o b a c c o [UNK] < e 1 e n d > [UNK] i n [UNK] M e x i c o ]. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 164, 150, 184, 187, 189, 170, 181, 178, 189, 194, 100, 189, 187, 174, 183, 173, 188, 100, 178, 183, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 172, 170, 183, 172, 174, 187, 100, 133, 174, 123, 174, 183, 173, 135, 100, 170, 189, 189, 187, 178, 171, 190, 189, 170, 171, 181, 174, 100, 189, 184, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 189, 184, 171, 170, 172, 172, 184, 100, 133, 174, 122, 174, 183, 173, 135, 100, 178, 183, 100, 150, 174, 193, 178, 172, 184, 166, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,219 - INFO - preprocess.py - convert_bert_example - 101 - ids：[63, 88, 22, 46]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 95 - *** train_example-4 ***
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] < e 1 s t a r t > [UNK] A r e c a [UNK] < e 1 e n d > [UNK] n u t [UNK] c h e w i n g [UNK] h a s [UNK] a [UNK] s i g n i f i c a n t [UNK] a s s o c i a t i o n [UNK] w i t h [UNK] < e 2 s t a r t > [UNK] s y s t e m i c [UNK] i n f l a m m a t i o n [UNK] < e 2 e n d >. [SEP]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 138, 187, 174, 172, 170, 100, 133, 174, 122, 174, 183, 173, 135, 100, 183, 190, 189, 100, 172, 177, 174, 192, 178, 183, 176, 100, 177, 170, 188, 100, 170, 100, 188, 178, 176, 183, 178, 175, 178, 172, 170, 183, 189, 100, 170, 188, 188, 184, 172, 178, 170, 189, 178, 184, 183, 100, 192, 178, 189, 177, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 188, 194, 188, 189, 174, 182, 178, 172, 100, 178, 183, 175, 181, 170, 182, 182, 170, 189, 178, 184, 183, 100, 133, 174, 123, 174, 183, 173, 135, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 100 - labels: 3
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 101 - ids：[1, 24, 72, 111]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 95 - *** train_example-5 ***
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] < e 2 s t a r t > [UNK] m a j o r [UNK] d e p r e s s i o n [UNK] < e 2 e n d > [UNK] ( M D ) [UNK] a n d [UNK] r e g u l a r [UNK] < e 1 s t a r t > [UNK] t o b a c c o [UNK] < e 1 e n d > [UNK] u s e [UNK] ( R U ) [UNK] o r [UNK] n i c o t i n e [UNK] d e p e n d e n c e [UNK] ( N D ). [SEP]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 182, 170, 179, 184, 187, 100, 173, 174, 185, 187, 174, 188, 188, 178, 184, 183, 100, 133, 174, 123, 174, 183, 173, 135, 100, 113, 150, 141, 114, 100, 170, 183, 173, 100, 187, 174, 176, 190, 181, 170, 187, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 189, 184, 171, 170, 172, 172, 184, 100, 133, 174, 122, 174, 183, 173, 135, 100, 190, 188, 174, 100, 113, 155, 158, 114, 100, 184, 187, 100, 183, 178, 172, 184, 189, 178, 183, 174, 100, 173, 174, 185, 174, 183, 173, 174, 183, 172, 174, 100, 113, 151, 141, 114, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 100 - labels: 3
2022-06-01 20:53:36,234 - INFO - preprocess.py - convert_bert_example - 101 - ids：[53, 78, 1, 35]
2022-06-01 20:53:36,250 - INFO - preprocess.py - convert_examples_to_features - 134 - Build 46 features
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 157 - example_text : Its effect on <e1start> digitalis <e1end>-caused <e2start> atrial arrhythmias <e2end> is unknown. 
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 159 - example_id_tags : [14, 41, 49, 85]
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 157 - example_text : However, the growth rate of <e2start> tumors <e2end> was not markedly inhibited by <e1start> garlic <e1end>. 
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 158 - example_id_label : Negative
2022-06-01 20:53:36,266 - INFO - preprocess.py - get_out - 159 - example_id_tags : [83, 107, 28, 52]
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 157 - example_text : <e1start> Tobacco <e1end>-related <e2start> cancers <e2end> in Madras, India.  
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 159 - example_id_tags : [0, 25, 34, 59]
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 157 - example_text : The importance of the <e1start> pecan <e1end> tree pollen in <e2start> allergic <e2end> manifestations.  
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 159 - example_id_tags : [22, 45, 61, 87]
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 157 - example_text : Components of <e1start> olive <e1end> oil and chemoprevention of <e2start> colorectal cancer <e2end>.  
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 158 - example_id_label : Negative
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 159 - example_id_tags : [14, 37, 65, 100]
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 157 - example_text : It is unlikely that <e1start> garlic <e1end> is useful in preventing <e2start> cardiovascular disease <e2end>.  
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 158 - example_id_label : Negative
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 159 - example_id_tags : [20, 44, 69, 109]
2022-06-01 20:53:36,281 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_examples_to_features - 120 - Convert 11 examples to features
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 95 - *** dev_example-0 ***
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] I t s [UNK] e f f e c t [UNK] o n [UNK] < e 1 s t a r t > [UNK] d i g i t a l i s [UNK] < e 1 e n d > - c a u s e d [UNK] < e 2 s t a r t > [UNK] a t r i a l [UNK] a r r h y t h m i a s [UNK] < e 2 e n d > [UNK] i s [UNK] u n k n o w n. [UNK] [SEP]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 146, 189, 188, 100, 174, 175, 175, 174, 172, 189, 100, 184, 183, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 173, 178, 176, 178, 189, 170, 181, 178, 188, 100, 133, 174, 122, 174, 183, 173, 135, 118, 172, 170, 190, 188, 174, 173, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 170, 189, 187, 178, 170, 181, 100, 170, 187, 187, 177, 194, 189, 177, 182, 178, 170, 188, 100, 133, 174, 123, 174, 183, 173, 135, 100, 178, 188, 100, 190, 183, 180, 183, 184, 192, 183, 119, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 101 - ids：[15, 42, 50, 86]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 95 - *** dev_example-1 ***
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] H o w e v e r, [UNK] t h e [UNK] g r o w t h [UNK] r a t e [UNK] o f [UNK] < e 2 s t a r t > [UNK] t u m o r s [UNK] < e 2 e n d > [UNK] w a s [UNK] n o t [UNK] m a r k e d l y [UNK] i n h i b i t e d [UNK] b y [UNK] < e 1 s t a r t > [UNK] g a r l i c [UNK] < e 1 e n d >. [UNK] [SEP]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 145, 184, 192, 174, 191, 174, 187, 117, 100, 189, 177, 174, 100, 176, 187, 184, 192, 189, 177, 100, 187, 170, 189, 174, 100, 184, 175, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 189, 190, 182, 184, 187, 188, 100, 133, 174, 123, 174, 183, 173, 135, 100, 192, 170, 188, 100, 183, 184, 189, 100, 182, 170, 187, 180, 174, 173, 181, 194, 100, 178, 183, 177, 178, 171, 178, 189, 174, 173, 100, 171, 194, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 176, 170, 187, 181, 178, 172, 100, 133, 174, 122, 174, 183, 173, 135, 119, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 100 - labels: 0
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 101 - ids：[84, 108, 29, 53]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 95 - *** dev_example-2 ***
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] < e 1 s t a r t > [UNK] T o b a c c o [UNK] < e 1 e n d > - r e l a t e d [UNK] < e 2 s t a r t > [UNK] c a n c e r s [UNK] < e 2 e n d > [UNK] i n [UNK] M a d r a s, [UNK] I n d i a. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,359 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 157, 184, 171, 170, 172, 172, 184, 100, 133, 174, 122, 174, 183, 173, 135, 118, 187, 174, 181, 170, 189, 174, 173, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 172, 170, 183, 172, 174, 187, 188, 100, 133, 174, 123, 174, 183, 173, 135, 100, 178, 183, 100, 150, 170, 173, 187, 170, 188, 117, 100, 146, 183, 173, 178, 170, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 101 - ids：[1, 26, 35, 60]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 95 - *** dev_example-3 ***
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] T h e [UNK] i m p o r t a n c e [UNK] o f [UNK] t h e [UNK] < e 1 s t a r t > [UNK] p e c a n [UNK] < e 1 e n d > [UNK] t r e e [UNK] p o l l e n [UNK] i n [UNK] < e 2 s t a r t > [UNK] a l l e r g i c [UNK] < e 2 e n d > [UNK] m a n i f e s t a t i o n s. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 157, 177, 174, 100, 178, 182, 185, 184, 187, 189, 170, 183, 172, 174, 100, 184, 175, 100, 189, 177, 174, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 185, 174, 172, 170, 183, 100, 133, 174, 122, 174, 183, 173, 135, 100, 189, 187, 174, 174, 100, 185, 184, 181, 181, 174, 183, 100, 178, 183, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 170, 181, 181, 174, 187, 176, 178, 172, 100, 133, 174, 123, 174, 183, 173, 135, 100, 182, 170, 183, 178, 175, 174, 188, 189, 170, 189, 178, 184, 183, 188, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 101 - ids：[23, 46, 62, 88]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 95 - *** dev_example-4 ***
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] C o m p o n e n t s [UNK] o f [UNK] < e 1 s t a r t > [UNK] o l i v e [UNK] < e 1 e n d > [UNK] o i l [UNK] a n d [UNK] c h e m o p r e v e n t i o n [UNK] o f [UNK] < e 2 s t a r t > [UNK] c o l o r e c t a l [UNK] c a n c e r [UNK] < e 2 e n d >. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 140, 184, 182, 185, 184, 183, 174, 183, 189, 188, 100, 184, 175, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 184, 181, 178, 191, 174, 100, 133, 174, 122, 174, 183, 173, 135, 100, 184, 178, 181, 100, 170, 183, 173, 100, 172, 177, 174, 182, 184, 185, 187, 174, 191, 174, 183, 189, 178, 184, 183, 100, 184, 175, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 172, 184, 181, 184, 187, 174, 172, 189, 170, 181, 100, 172, 170, 183, 172, 174, 187, 100, 133, 174, 123, 174, 183, 173, 135, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 100 - labels: 0
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 101 - ids：[15, 38, 66, 101]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 95 - *** dev_example-5 ***
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] I t [UNK] i s [UNK] u n l i k e l y [UNK] t h a t [UNK] < e 1 s t a r t > [UNK] g a r l i c [UNK] < e 1 e n d > [UNK] i s [UNK] u s e f u l [UNK] i n [UNK] p r e v e n t i n g [UNK] < e 2 s t a r t > [UNK] c a r d i o v a s c u l a r [UNK] d i s e a s e [UNK] < e 2 e n d >. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,375 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 146, 189, 100, 178, 188, 100, 190, 183, 181, 178, 180, 174, 181, 194, 100, 189, 177, 170, 189, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 176, 170, 187, 181, 178, 172, 100, 133, 174, 122, 174, 183, 173, 135, 100, 178, 188, 100, 190, 188, 174, 175, 190, 181, 100, 178, 183, 100, 185, 187, 174, 191, 174, 183, 189, 178, 183, 176, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 172, 170, 187, 173, 178, 184, 191, 170, 188, 172, 190, 181, 170, 187, 100, 173, 178, 188, 174, 170, 188, 174, 100, 133, 174, 123, 174, 183, 173, 135, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,391 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,391 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,391 - INFO - preprocess.py - convert_bert_example - 100 - labels: 0
2022-06-01 20:53:36,391 - INFO - preprocess.py - convert_bert_example - 101 - ids：[21, 45, 70, 110]
2022-06-01 20:53:36,391 - INFO - preprocess.py - convert_examples_to_features - 134 - Build 11 features
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 157 - example_text : Its effect on <e1start> digitalis <e1end>-caused <e2start> atrial arrhythmias <e2end> is unknown. 
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 159 - example_id_tags : [14, 41, 49, 85]
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 157 - example_text : However, the growth rate of <e2start> tumors <e2end> was not markedly inhibited by <e1start> garlic <e1end>. 
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 158 - example_id_label : Negative
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 159 - example_id_tags : [83, 107, 28, 52]
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 157 - example_text : <e1start> Tobacco <e1end>-related <e2start> cancers <e2end> in Madras, India.  
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 159 - example_id_tags : [0, 25, 34, 59]
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,391 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 157 - example_text : The importance of the <e1start> pecan <e1end> tree pollen in <e2start> allergic <e2end> manifestations.  
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 158 - example_id_label : Cause_of_disease
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 159 - example_id_tags : [22, 45, 61, 87]
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 157 - example_text : Components of <e1start> olive <e1end> oil and chemoprevention of <e2start> colorectal cancer <e2end>.  
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 158 - example_id_label : Negative
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 159 - example_id_tags : [14, 37, 65, 100]
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 156 - ==========================
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 157 - example_text : It is unlikely that <e1start> garlic <e1end> is useful in preventing <e2start> cardiovascular disease <e2end>.  
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 158 - example_id_label : Negative
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 159 - example_id_tags : [20, 44, 69, 109]
2022-06-01 20:53:36,406 - INFO - preprocess.py - get_out - 160 - ==========================
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_examples_to_features - 120 - Convert 11 examples to features
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 95 - *** test_example-0 ***
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] I t s [UNK] e f f e c t [UNK] o n [UNK] < e 1 s t a r t > [UNK] d i g i t a l i s [UNK] < e 1 e n d > - c a u s e d [UNK] < e 2 s t a r t > [UNK] a t r i a l [UNK] a r r h y t h m i a s [UNK] < e 2 e n d > [UNK] i s [UNK] u n k n o w n. [UNK] [SEP]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 146, 189, 188, 100, 174, 175, 175, 174, 172, 189, 100, 184, 183, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 173, 178, 176, 178, 189, 170, 181, 178, 188, 100, 133, 174, 122, 174, 183, 173, 135, 118, 172, 170, 190, 188, 174, 173, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 170, 189, 187, 178, 170, 181, 100, 170, 187, 187, 177, 194, 189, 177, 182, 178, 170, 188, 100, 133, 174, 123, 174, 183, 173, 135, 100, 178, 188, 100, 190, 183, 180, 183, 184, 192, 183, 119, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 101 - ids：[15, 42, 50, 86]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 95 - *** test_example-1 ***
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] H o w e v e r, [UNK] t h e [UNK] g r o w t h [UNK] r a t e [UNK] o f [UNK] < e 2 s t a r t > [UNK] t u m o r s [UNK] < e 2 e n d > [UNK] w a s [UNK] n o t [UNK] m a r k e d l y [UNK] i n h i b i t e d [UNK] b y [UNK] < e 1 s t a r t > [UNK] g a r l i c [UNK] < e 1 e n d >. [UNK] [SEP]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 145, 184, 192, 174, 191, 174, 187, 117, 100, 189, 177, 174, 100, 176, 187, 184, 192, 189, 177, 100, 187, 170, 189, 174, 100, 184, 175, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 189, 190, 182, 184, 187, 188, 100, 133, 174, 123, 174, 183, 173, 135, 100, 192, 170, 188, 100, 183, 184, 189, 100, 182, 170, 187, 180, 174, 173, 181, 194, 100, 178, 183, 177, 178, 171, 178, 189, 174, 173, 100, 171, 194, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 176, 170, 187, 181, 178, 172, 100, 133, 174, 122, 174, 183, 173, 135, 119, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 100 - labels: 0
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 101 - ids：[84, 108, 29, 53]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 95 - *** test_example-2 ***
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] < e 1 s t a r t > [UNK] T o b a c c o [UNK] < e 1 e n d > - r e l a t e d [UNK] < e 2 s t a r t > [UNK] c a n c e r s [UNK] < e 2 e n d > [UNK] i n [UNK] M a d r a s, [UNK] I n d i a. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,469 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 157, 184, 171, 170, 172, 172, 184, 100, 133, 174, 122, 174, 183, 173, 135, 118, 187, 174, 181, 170, 189, 174, 173, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 172, 170, 183, 172, 174, 187, 188, 100, 133, 174, 123, 174, 183, 173, 135, 100, 178, 183, 100, 150, 170, 173, 187, 170, 188, 117, 100, 146, 183, 173, 178, 170, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 101 - ids：[1, 26, 35, 60]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 95 - *** test_example-3 ***
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] T h e [UNK] i m p o r t a n c e [UNK] o f [UNK] t h e [UNK] < e 1 s t a r t > [UNK] p e c a n [UNK] < e 1 e n d > [UNK] t r e e [UNK] p o l l e n [UNK] i n [UNK] < e 2 s t a r t > [UNK] a l l e r g i c [UNK] < e 2 e n d > [UNK] m a n i f e s t a t i o n s. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 157, 177, 174, 100, 178, 182, 185, 184, 187, 189, 170, 183, 172, 174, 100, 184, 175, 100, 189, 177, 174, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 185, 174, 172, 170, 183, 100, 133, 174, 122, 174, 183, 173, 135, 100, 189, 187, 174, 174, 100, 185, 184, 181, 181, 174, 183, 100, 178, 183, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 170, 181, 181, 174, 187, 176, 178, 172, 100, 133, 174, 123, 174, 183, 173, 135, 100, 182, 170, 183, 178, 175, 174, 188, 189, 170, 189, 178, 184, 183, 188, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 100 - labels: 1
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 101 - ids：[23, 46, 62, 88]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 95 - *** test_example-4 ***
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] C o m p o n e n t s [UNK] o f [UNK] < e 1 s t a r t > [UNK] o l i v e [UNK] < e 1 e n d > [UNK] o i l [UNK] a n d [UNK] c h e m o p r e v e n t i o n [UNK] o f [UNK] < e 2 s t a r t > [UNK] c o l o r e c t a l [UNK] c a n c e r [UNK] < e 2 e n d >. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 140, 184, 182, 185, 184, 183, 174, 183, 189, 188, 100, 184, 175, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 184, 181, 178, 191, 174, 100, 133, 174, 122, 174, 183, 173, 135, 100, 184, 178, 181, 100, 170, 183, 173, 100, 172, 177, 174, 182, 184, 185, 187, 174, 191, 174, 183, 189, 178, 184, 183, 100, 184, 175, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 172, 184, 181, 184, 187, 174, 172, 189, 170, 181, 100, 172, 170, 183, 172, 174, 187, 100, 133, 174, 123, 174, 183, 173, 135, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 100 - labels: 0
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 101 - ids：[15, 38, 66, 101]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 95 - *** test_example-5 ***
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 96 - text: [CLS] I t [UNK] i s [UNK] u n l i k e l y [UNK] t h a t [UNK] < e 1 s t a r t > [UNK] g a r l i c [UNK] < e 1 e n d > [UNK] i s [UNK] u s e f u l [UNK] i n [UNK] p r e v e n t i n g [UNK] < e 2 s t a r t > [UNK] c a r d i o v a s c u l a r [UNK] d i s e a s e [UNK] < e 2 e n d >. [UNK] [UNK] [SEP]
2022-06-01 20:53:36,484 - INFO - preprocess.py - convert_bert_example - 97 - token_ids: [101, 146, 189, 100, 178, 188, 100, 190, 183, 181, 178, 180, 174, 181, 194, 100, 189, 177, 170, 189, 100, 133, 174, 122, 188, 189, 170, 187, 189, 135, 100, 176, 170, 187, 181, 178, 172, 100, 133, 174, 122, 174, 183, 173, 135, 100, 178, 188, 100, 190, 188, 174, 175, 190, 181, 100, 178, 183, 100, 185, 187, 174, 191, 174, 183, 189, 178, 183, 176, 100, 133, 174, 123, 188, 189, 170, 187, 189, 135, 100, 172, 170, 187, 173, 178, 184, 191, 170, 188, 172, 190, 181, 170, 187, 100, 173, 178, 188, 174, 170, 188, 174, 100, 133, 174, 123, 174, 183, 173, 135, 119, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,516 - INFO - preprocess.py - convert_bert_example - 98 - attention_masks: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,516 - INFO - preprocess.py - convert_bert_example - 99 - token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
2022-06-01 20:53:36,516 - INFO - preprocess.py - convert_bert_example - 100 - labels: 0
2022-06-01 20:53:36,516 - INFO - preprocess.py - convert_bert_example - 101 - ids：[21, 45, 70, 110]
2022-06-01 20:53:36,516 - INFO - preprocess.py - convert_examples_to_features - 134 - Build 11 features
2022-06-01 20:53:43,141 - INFO - main.py - <module> - 248 - ======== Training And Validation========
2022-06-01 20:54:48,267 - INFO - main.py - train - 87 - 【train】 epoch：0 step:0/200 loss：1.398286
2022-06-01 20:55:05,908 - INFO - main.py - train - 87 - 【train】 epoch：0 step:1/200 loss：1.479077
2022-06-01 20:57:47,662 - INFO - main.py - train - 87 - 【train】 epoch：1 step:2/200 loss：1.316834
2022-06-01 20:58:08,276 - INFO - main.py - train - 87 - 【train】 epoch：1 step:3/200 loss：1.202168
2022-06-01 21:01:09,751 - INFO - main.py - train - 87 - 【train】 epoch：2 step:4/200 loss：1.179474
2022-06-01 21:01:29,298 - INFO - main.py - train - 87 - 【train】 epoch：2 step:5/200 loss：1.288168
2022-06-01 21:03:52,429 - INFO - main.py - train - 87 - 【train】 epoch：3 step:6/200 loss：1.057614
2022-06-01 21:04:10,796 - INFO - main.py - train - 87 - 【train】 epoch：3 step:7/200 loss：1.178768
2022-06-01 21:06:02,746 - INFO - main.py - train - 87 - 【train】 epoch：4 step:8/200 loss：1.082099
2022-06-01 21:06:21,242 - INFO - main.py - train - 87 - 【train】 epoch：4 step:9/200 loss：0.915970
2022-06-01 21:08:03,786 - INFO - main.py - train - 87 - 【train】 epoch：5 step:10/200 loss：0.883701
2022-06-01 21:08:20,770 - INFO - main.py - train - 87 - 【train】 epoch：5 step:11/200 loss：0.910001
2022-06-01 21:09:53,296 - INFO - main.py - train - 87 - 【train】 epoch：6 step:12/200 loss：0.866560
2022-06-01 21:10:10,921 - INFO - main.py - train - 87 - 【train】 epoch：6 step:13/200 loss：0.762957
2022-06-01 21:11:48,260 - INFO - main.py - train - 87 - 【train】 epoch：7 step:14/200 loss：0.859775
2022-06-01 21:12:06,017 - INFO - main.py - train - 87 - 【train】 epoch：7 step:15/200 loss：0.519851
2022-06-01 21:13:36,404 - INFO - main.py - train - 87 - 【train】 epoch：8 step:16/200 loss：0.704084
2022-06-01 21:13:54,388 - INFO - main.py - train - 87 - 【train】 epoch：8 step:17/200 loss：0.596629
2022-06-01 21:15:48,603 - INFO - main.py - train - 87 - 【train】 epoch：9 step:18/200 loss：0.564968
2022-06-01 21:16:05,696 - INFO - main.py - train - 87 - 【train】 epoch：9 step:19/200 loss：0.642278
2022-06-01 21:17:50,139 - INFO - main.py - train - 87 - 【train】 epoch：10 step:20/200 loss：0.562932
2022-06-01 21:18:08,764 - INFO - main.py - train - 87 - 【train】 epoch：10 step:21/200 loss：0.596987
2022-06-01 21:20:06,618 - INFO - main.py - train - 87 - 【train】 epoch：11 step:22/200 loss：0.551757
2022-06-01 21:20:24,477 - INFO - main.py - train - 87 - 【train】 epoch：11 step:23/200 loss：0.524080
2022-06-01 21:22:23,572 - INFO - main.py - train - 87 - 【train】 epoch：12 step:24/200 loss：0.429540
2022-06-01 21:22:41,322 - INFO - main.py - train - 87 - 【train】 epoch：12 step:25/200 loss：0.439745
2022-06-01 21:24:36,182 - INFO - main.py - train - 87 - 【train】 epoch：13 step:26/200 loss：0.369495
2022-06-01 21:24:53,855 - INFO - main.py - train - 87 - 【train】 epoch：13 step:27/200 loss：0.336853
2022-06-01 21:26:34,645 - INFO - main.py - train - 87 - 【train】 epoch：14 step:28/200 loss：0.316170
2022-06-01 21:26:52,286 - INFO - main.py - train - 87 - 【train】 epoch：14 step:29/200 loss：0.226973
2022-06-01 21:28:49,567 - INFO - main.py - train - 87 - 【train】 epoch：15 step:30/200 loss：0.261927
2022-06-01 21:29:07,504 - INFO - main.py - train - 87 - 【train】 epoch：15 step:31/200 loss：0.327042
2022-06-01 21:30:53,856 - INFO - main.py - train - 87 - 【train】 epoch：16 step:32/200 loss：0.282684
2022-06-01 21:31:11,341 - INFO - main.py - train - 87 - 【train】 epoch：16 step:33/200 loss：0.139775
2022-06-01 21:32:54,339 - INFO - main.py - train - 87 - 【train】 epoch：17 step:34/200 loss：0.197837
2022-06-01 21:33:15,602 - INFO - main.py - train - 87 - 【train】 epoch：17 step:35/200 loss：0.197992
2022-06-01 21:34:56,899 - INFO - main.py - train - 87 - 【train】 epoch：18 step:36/200 loss：0.200799
2022-06-01 21:35:15,727 - INFO - main.py - train - 87 - 【train】 epoch：18 step:37/200 loss：0.081312
2022-06-01 21:36:56,662 - INFO - main.py - train - 87 - 【train】 epoch：19 step:38/200 loss：0.130379
2022-06-01 21:37:14,397 - INFO - main.py - train - 87 - 【train】 epoch：19 step:39/200 loss：0.163634
2022-06-01 21:39:19,994 - INFO - main.py - train - 87 - 【train】 epoch：20 step:40/200 loss：0.081903
2022-06-01 21:39:36,791 - INFO - main.py - train - 87 - 【train】 epoch：20 step:41/200 loss：0.116372
2022-06-01 21:41:29,077 - INFO - main.py - train - 87 - 【train】 epoch：21 step:42/200 loss：0.109735
2022-06-01 21:41:46,249 - INFO - main.py - train - 87 - 【train】 epoch：21 step:43/200 loss：0.107802
2022-06-01 21:43:38,793 - INFO - main.py - train - 87 - 【train】 epoch：22 step:44/200 loss：0.092978
2022-06-01 21:43:56,862 - INFO - main.py - train - 87 - 【train】 epoch：22 step:45/200 loss：0.080604
2022-06-01 21:45:48,716 - INFO - main.py - train - 87 - 【train】 epoch：23 step:46/200 loss：0.084961
2022-06-01 21:46:05,685 - INFO - main.py - train - 87 - 【train】 epoch：23 step:47/200 loss：0.072373
2022-06-01 21:47:42,207 - INFO - main.py - train - 87 - 【train】 epoch：24 step:48/200 loss：0.060640
2022-06-01 21:48:00,457 - INFO - main.py - train - 87 - 【train】 epoch：24 step:49/200 loss：0.073771
2022-06-01 21:50:07,609 - INFO - main.py - train - 87 - 【train】 epoch：25 step:50/200 loss：0.039402
2022-06-01 21:50:25,375 - INFO - main.py - train - 87 - 【train】 epoch：25 step:51/200 loss：0.074169
2022-06-01 21:52:13,728 - INFO - main.py - train - 87 - 【train】 epoch：26 step:52/200 loss：0.042913
2022-06-01 21:52:32,368 - INFO - main.py - train - 87 - 【train】 epoch：26 step:53/200 loss：0.035213
2022-06-01 21:54:17,935 - INFO - main.py - train - 87 - 【train】 epoch：27 step:54/200 loss：0.042833
2022-06-01 21:54:35,356 - INFO - main.py - train - 87 - 【train】 epoch：27 step:55/200 loss：0.030803
2022-06-01 21:56:40,161 - INFO - main.py - train - 87 - 【train】 epoch：28 step:56/200 loss：0.032061
2022-06-01 21:56:58,614 - INFO - main.py - train - 87 - 【train】 epoch：28 step:57/200 loss：0.029119
2022-06-01 21:59:14,994 - INFO - main.py - train - 87 - 【train】 epoch：29 step:58/200 loss：0.036846
2022-06-01 21:59:33,869 - INFO - main.py - train - 87 - 【train】 epoch：29 step:59/200 loss：0.020882
2022-06-01 22:01:20,960 - INFO - main.py - train - 87 - 【train】 epoch：30 step:60/200 loss：0.024858
2022-06-01 22:01:40,257 - INFO - main.py - train - 87 - 【train】 epoch：30 step:61/200 loss：0.034905
2022-06-01 22:03:48,951 - INFO - main.py - train - 87 - 【train】 epoch：31 step:62/200 loss：0.020206
2022-06-01 22:04:06,904 - INFO - main.py - train - 87 - 【train】 epoch：31 step:63/200 loss：0.025363
2022-06-01 22:05:50,505 - INFO - main.py - train - 87 - 【train】 epoch：32 step:64/200 loss：0.022077
2022-06-01 22:06:08,693 - INFO - main.py - train - 87 - 【train】 epoch：32 step:65/200 loss：0.014075
2022-06-01 22:07:52,313 - INFO - main.py - train - 87 - 【train】 epoch：33 step:66/200 loss：0.023336
2022-06-01 22:08:10,891 - INFO - main.py - train - 87 - 【train】 epoch：33 step:67/200 loss：0.013232
2022-06-01 22:10:15,317 - INFO - main.py - train - 87 - 【train】 epoch：34 step:68/200 loss：0.017236
2022-06-01 22:10:33,958 - INFO - main.py - train - 87 - 【train】 epoch：34 step:69/200 loss：0.012075
2022-06-01 22:12:26,966 - INFO - main.py - train - 87 - 【train】 epoch：35 step:70/200 loss：0.012706
2022-06-01 22:12:44,857 - INFO - main.py - train - 87 - 【train】 epoch：35 step:71/200 loss：0.014752
2022-06-01 22:15:01,264 - INFO - main.py - train - 87 - 【train】 epoch：36 step:72/200 loss：0.010835
2022-06-01 22:15:19,279 - INFO - main.py - train - 87 - 【train】 epoch：36 step:73/200 loss：0.012665
2022-06-01 22:17:28,068 - INFO - main.py - train - 87 - 【train】 epoch：37 step:74/200 loss：0.009576
2022-06-01 22:17:46,489 - INFO - main.py - train - 87 - 【train】 epoch：37 step:75/200 loss：0.013878
2022-06-01 22:20:11,002 - INFO - main.py - train - 87 - 【train】 epoch：38 step:76/200 loss：0.009643
2022-06-01 22:20:28,940 - INFO - main.py - train - 87 - 【train】 epoch：38 step:77/200 loss：0.009931
2022-06-01 22:22:23,350 - INFO - main.py - train - 87 - 【train】 epoch：39 step:78/200 loss：0.008110
2022-06-01 22:22:41,288 - INFO - main.py - train - 87 - 【train】 epoch：39 step:79/200 loss：0.008286
2022-06-01 22:24:30,451 - INFO - main.py - train - 87 - 【train】 epoch：40 step:80/200 loss：0.007825
2022-06-01 22:24:48,560 - INFO - main.py - train - 87 - 【train】 epoch：40 step:81/200 loss：0.007953
2022-06-01 22:26:33,659 - INFO - main.py - train - 87 - 【train】 epoch：41 step:82/200 loss：0.007986
2022-06-01 22:26:52,061 - INFO - main.py - train - 87 - 【train】 epoch：41 step:83/200 loss：0.006301
2022-06-01 22:28:52,011 - INFO - main.py - train - 87 - 【train】 epoch：42 step:84/200 loss：0.006151
2022-06-01 22:29:10,208 - INFO - main.py - train - 87 - 【train】 epoch：42 step:85/200 loss：0.007652
2022-06-01 22:31:19,648 - INFO - main.py - train - 87 - 【train】 epoch：43 step:86/200 loss：0.005893
2022-06-01 22:31:40,186 - INFO - main.py - train - 87 - 【train】 epoch：43 step:87/200 loss：0.007590
2022-06-01 22:33:48,837 - INFO - main.py - train - 87 - 【train】 epoch：44 step:88/200 loss：0.005845
2022-06-01 22:34:07,134 - INFO - main.py - train - 87 - 【train】 epoch：44 step:89/200 loss：0.008002
2022-06-01 22:36:06,750 - INFO - main.py - train - 87 - 【train】 epoch：45 step:90/200 loss：0.005646
2022-06-01 22:36:25,265 - INFO - main.py - train - 87 - 【train】 epoch：45 step:91/200 loss：0.005563
2022-06-01 22:38:47,304 - INFO - main.py - train - 87 - 【train】 epoch：46 step:92/200 loss：0.004987
2022-06-01 22:39:05,085 - INFO - main.py - train - 87 - 【train】 epoch：46 step:93/200 loss：0.004747
2022-06-01 22:41:20,353 - INFO - main.py - train - 87 - 【train】 epoch：47 step:94/200 loss：0.004781
2022-06-01 22:41:38,186 - INFO - main.py - train - 87 - 【train】 epoch：47 step:95/200 loss：0.005331
2022-06-01 22:43:34,532 - INFO - main.py - train - 87 - 【train】 epoch：48 step:96/200 loss：0.005098
2022-06-01 22:43:53,532 - INFO - main.py - train - 87 - 【train】 epoch：48 step:97/200 loss：0.003799
2022-06-01 22:45:50,454 - INFO - main.py - train - 87 - 【train】 epoch：49 step:98/200 loss：0.005164
2022-06-01 22:46:09,282 - INFO - main.py - train - 87 - 【train】 epoch：49 step:99/200 loss：0.005021
2022-06-01 22:47:01,244 - INFO - main.py - train - 93 - 【dev】 loss：2.787870 accuracy：0.3636 micro_f1：0.3636 macro_f1：0.4325
2022-06-01 22:47:01,276 - INFO - main.py - train - 96 - ------------>Save best model
2022-06-01 22:48:35,335 - INFO - main.py - train - 87 - 【train】 epoch：50 step:100/200 loss：0.004737
2022-06-01 22:48:54,273 - INFO - main.py - train - 87 - 【train】 epoch：50 step:101/200 loss：0.003588
2022-06-01 22:50:44,209 - INFO - main.py - train - 87 - 【train】 epoch：51 step:102/200 loss：0.005545
2022-06-01 22:51:02,897 - INFO - main.py - train - 87 - 【train】 epoch：51 step:103/200 loss：0.004395
2022-06-01 22:52:59,669 - INFO - main.py - train - 87 - 【train】 epoch：52 step:104/200 loss：0.004684
2022-06-01 22:53:19,435 - INFO - main.py - train - 87 - 【train】 epoch：52 step:105/200 loss：0.004553
2022-06-01 22:55:20,256 - INFO - main.py - train - 87 - 【train】 epoch：53 step:106/200 loss：0.004425
2022-06-01 22:55:39,271 - INFO - main.py - train - 87 - 【train】 epoch：53 step:107/200 loss：0.003618
2022-06-01 22:57:27,384 - INFO - main.py - train - 87 - 【train】 epoch：54 step:108/200 loss：0.004605
2022-06-01 22:57:45,869 - INFO - main.py - train - 87 - 【train】 epoch：54 step:109/200 loss：0.005116
2022-06-01 22:59:59,741 - INFO - main.py - train - 87 - 【train】 epoch：55 step:110/200 loss：0.005766
2022-06-01 23:00:17,850 - INFO - main.py - train - 87 - 【train】 epoch：55 step:111/200 loss：0.002589
2022-06-01 23:02:02,015 - INFO - main.py - train - 87 - 【train】 epoch：56 step:112/200 loss：0.003505
2022-06-01 23:02:24,594 - INFO - main.py - train - 87 - 【train】 epoch：56 step:113/200 loss：0.003256
2022-06-01 23:04:30,249 - INFO - main.py - train - 87 - 【train】 epoch：57 step:114/200 loss：0.004316
2022-06-01 23:04:49,749 - INFO - main.py - train - 87 - 【train】 epoch：57 step:115/200 loss：0.003215
2022-06-01 23:06:44,674 - INFO - main.py - train - 87 - 【train】 epoch：58 step:116/200 loss：0.003888
2022-06-01 23:07:03,815 - INFO - main.py - train - 87 - 【train】 epoch：58 step:117/200 loss：0.004138
2022-06-01 23:09:20,901 - INFO - main.py - train - 87 - 【train】 epoch：59 step:118/200 loss：0.003042
2022-06-01 23:09:40,026 - INFO - main.py - train - 87 - 【train】 epoch：59 step:119/200 loss：0.003907
2022-06-01 23:11:36,974 - INFO - main.py - train - 87 - 【train】 epoch：60 step:120/200 loss：0.003707
2022-06-01 23:11:54,896 - INFO - main.py - train - 87 - 【train】 epoch：60 step:121/200 loss：0.003710
2022-06-01 23:13:40,805 - INFO - main.py - train - 87 - 【train】 epoch：61 step:122/200 loss：0.003840
2022-06-01 23:14:03,993 - INFO - main.py - train - 87 - 【train】 epoch：61 step:123/200 loss：0.003034
2022-06-01 23:16:37,461 - INFO - main.py - train - 87 - 【train】 epoch：62 step:124/200 loss：0.003377
2022-06-01 23:16:57,274 - INFO - main.py - train - 87 - 【train】 epoch：62 step:125/200 loss：0.003777
2022-06-01 23:19:08,887 - INFO - main.py - train - 87 - 【train】 epoch：63 step:126/200 loss：0.003639
2022-06-01 23:19:27,293 - INFO - main.py - train - 87 - 【train】 epoch：63 step:127/200 loss：0.004000
2022-06-01 23:21:19,837 - INFO - main.py - train - 87 - 【train】 epoch：64 step:128/200 loss：0.002950
2022-06-01 23:21:40,477 - INFO - main.py - train - 87 - 【train】 epoch：64 step:129/200 loss：0.003459
2022-06-01 23:24:11,877 - INFO - main.py - train - 87 - 【train】 epoch：65 step:130/200 loss：0.003160
2022-06-01 23:24:31,398 - INFO - main.py - train - 87 - 【train】 epoch：65 step:131/200 loss：0.004238
2022-06-01 23:26:33,621 - INFO - main.py - train - 87 - 【train】 epoch：66 step:132/200 loss：0.004074
2022-06-01 23:26:52,897 - INFO - main.py - train - 87 - 【train】 epoch：66 step:133/200 loss：0.003621
2022-06-01 23:28:43,841 - INFO - main.py - train - 87 - 【train】 epoch：67 step:134/200 loss：0.003019
2022-06-01 23:29:03,169 - INFO - main.py - train - 87 - 【train】 epoch：67 step:135/200 loss：0.003016
2022-06-01 23:30:51,374 - INFO - main.py - train - 87 - 【train】 epoch：68 step:136/200 loss：0.003431
2022-06-01 23:31:11,155 - INFO - main.py - train - 87 - 【train】 epoch：68 step:137/200 loss：0.003027
2022-06-01 23:33:26,724 - INFO - main.py - train - 87 - 【train】 epoch：69 step:138/200 loss：0.002585
2022-06-01 23:33:45,520 - INFO - main.py - train - 87 - 【train】 epoch：69 step:139/200 loss：0.002409
2022-06-01 23:35:32,496 - INFO - main.py - train - 87 - 【train】 epoch：70 step:140/200 loss：0.002845
2022-06-01 23:35:50,503 - INFO - main.py - train - 87 - 【train】 epoch：70 step:141/200 loss：0.002621
2022-06-01 23:37:51,518 - INFO - main.py - train - 87 - 【train】 epoch：71 step:142/200 loss：0.003271
2022-06-01 23:38:08,909 - INFO - main.py - train - 87 - 【train】 epoch：71 step:143/200 loss：0.002617
2022-06-01 23:39:54,676 - INFO - main.py - train - 87 - 【train】 epoch：72 step:144/200 loss：0.002709
2022-06-01 23:40:12,832 - INFO - main.py - train - 87 - 【train】 epoch：72 step:145/200 loss：0.002590
2022-06-01 23:41:59,818 - INFO - main.py - train - 87 - 【train】 epoch：73 step:146/200 loss：0.003014
2022-06-01 23:42:18,895 - INFO - main.py - train - 87 - 【train】 epoch：73 step:147/200 loss：0.002538
2022-06-01 23:44:17,935 - INFO - main.py - train - 87 - 【train】 epoch：74 step:148/200 loss：0.003440
2022-06-01 23:44:35,810 - INFO - main.py - train - 87 - 【train】 epoch：74 step:149/200 loss：0.002282
2022-06-01 23:46:22,493 - INFO - main.py - train - 87 - 【train】 epoch：75 step:150/200 loss：0.002410
2022-06-01 23:46:40,368 - INFO - main.py - train - 87 - 【train】 epoch：75 step:151/200 loss：0.002293
2022-06-01 23:48:42,440 - INFO - main.py - train - 87 - 【train】 epoch：76 step:152/200 loss：0.002569
2022-06-01 23:49:01,603 - INFO - main.py - train - 87 - 【train】 epoch：76 step:153/200 loss：0.002023
2022-06-01 23:51:07,200 - INFO - main.py - train - 87 - 【train】 epoch：77 step:154/200 loss：0.002613
2022-06-01 23:51:24,966 - INFO - main.py - train - 87 - 【train】 epoch：77 step:155/200 loss：0.001645
2022-06-01 23:53:33,464 - INFO - main.py - train - 87 - 【train】 epoch：78 step:156/200 loss：0.002297
2022-06-01 23:53:51,011 - INFO - main.py - train - 87 - 【train】 epoch：78 step:157/200 loss：0.001743
2022-06-01 23:55:44,225 - INFO - main.py - train - 87 - 【train】 epoch：79 step:158/200 loss：0.002239
2022-06-01 23:56:03,294 - INFO - main.py - train - 87 - 【train】 epoch：79 step:159/200 loss：0.001628
2022-06-01 23:57:51,535 - INFO - main.py - train - 87 - 【train】 epoch：80 step:160/200 loss：0.002516
2022-06-01 23:58:11,332 - INFO - main.py - train - 87 - 【train】 epoch：80 step:161/200 loss：0.001564
2022-06-01 23:59:59,276 - INFO - main.py - train - 87 - 【train】 epoch：81 step:162/200 loss：0.002147
2022-06-02 00:00:19,338 - INFO - main.py - train - 87 - 【train】 epoch：81 step:163/200 loss：0.002714
2022-06-02 00:02:12,060 - INFO - main.py - train - 87 - 【train】 epoch：82 step:164/200 loss：0.002323
2022-06-02 00:02:30,639 - INFO - main.py - train - 87 - 【train】 epoch：82 step:165/200 loss：0.002082
2022-06-02 00:04:23,451 - INFO - main.py - train - 87 - 【train】 epoch：83 step:166/200 loss：0.002140
2022-06-02 00:04:42,545 - INFO - main.py - train - 87 - 【train】 epoch：83 step:167/200 loss：0.002118
2022-06-02 00:06:32,592 - INFO - main.py - train - 87 - 【train】 epoch：84 step:168/200 loss：0.002145
2022-06-02 00:06:52,952 - INFO - main.py - train - 87 - 【train】 epoch：84 step:169/200 loss：0.002013
2022-06-02 00:08:49,778 - INFO - main.py - train - 87 - 【train】 epoch：85 step:170/200 loss：0.002386
2022-06-02 00:09:09,052 - INFO - main.py - train - 87 - 【train】 epoch：85 step:171/200 loss：0.001539
2022-06-02 00:10:58,043 - INFO - main.py - train - 87 - 【train】 epoch：86 step:172/200 loss：0.002136
2022-06-02 00:11:16,152 - INFO - main.py - train - 87 - 【train】 epoch：86 step:173/200 loss：0.001673
2022-06-02 00:13:06,386 - INFO - main.py - train - 87 - 【train】 epoch：87 step:174/200 loss：0.001972
2022-06-02 00:13:25,323 - INFO - main.py - train - 87 - 【train】 epoch：87 step:175/200 loss：0.001681
2022-06-02 00:15:04,853 - INFO - main.py - train - 87 - 【train】 epoch：88 step:176/200 loss：0.002238
2022-06-02 00:15:23,337 - INFO - main.py - train - 87 - 【train】 epoch：88 step:177/200 loss：0.001174
2022-06-02 00:17:14,192 - INFO - main.py - train - 87 - 【train】 epoch：89 step:178/200 loss：0.001810
2022-06-02 00:17:32,583 - INFO - main.py - train - 87 - 【train】 epoch：89 step:179/200 loss：0.001655
2022-06-02 00:19:18,164 - INFO - main.py - train - 87 - 【train】 epoch：90 step:180/200 loss：0.001841
2022-06-02 00:19:36,445 - INFO - main.py - train - 87 - 【train】 epoch：90 step:181/200 loss：0.001635
2022-06-02 00:21:30,040 - INFO - main.py - train - 87 - 【train】 epoch：91 step:182/200 loss：0.001599
2022-06-02 00:21:50,415 - INFO - main.py - train - 87 - 【train】 epoch：91 step:183/200 loss：0.001882
2022-06-02 00:24:50,333 - INFO - main.py - train - 87 - 【train】 epoch：92 step:184/200 loss：0.001663
2022-06-02 00:25:10,161 - INFO - main.py - train - 87 - 【train】 epoch：92 step:185/200 loss：0.001913
2022-06-02 00:27:51,619 - INFO - main.py - train - 87 - 【train】 epoch：93 step:186/200 loss：0.001503
2022-06-02 00:28:11,462 - INFO - main.py - train - 87 - 【train】 epoch：93 step:187/200 loss：0.001567
2022-06-02 00:30:12,015 - INFO - main.py - train - 87 - 【train】 epoch：94 step:188/200 loss：0.001992
2022-06-02 00:30:29,965 - INFO - main.py - train - 87 - 【train】 epoch：94 step:189/200 loss：0.002094
2022-06-02 00:32:52,393 - INFO - main.py - train - 87 - 【train】 epoch：95 step:190/200 loss：0.001743
2022-06-02 00:33:12,142 - INFO - main.py - train - 87 - 【train】 epoch：95 step:191/200 loss：0.001708
2022-06-02 00:35:25,773 - INFO - main.py - train - 87 - 【train】 epoch：96 step:192/200 loss：0.001897
2022-06-02 00:35:48,256 - INFO - main.py - train - 87 - 【train】 epoch：96 step:193/200 loss：0.001638
2022-06-02 00:38:02,139 - INFO - main.py - train - 87 - 【train】 epoch：97 step:194/200 loss：0.001700
2022-06-02 00:38:19,686 - INFO - main.py - train - 87 - 【train】 epoch：97 step:195/200 loss：0.001449
2022-06-02 00:40:15,885 - INFO - main.py - train - 87 - 【train】 epoch：98 step:196/200 loss：0.001986
2022-06-02 00:40:35,229 - INFO - main.py - train - 87 - 【train】 epoch：98 step:197/200 loss：0.001424
2022-06-02 00:42:12,355 - INFO - main.py - train - 87 - 【train】 epoch：99 step:198/200 loss：0.002007
2022-06-02 00:42:30,152 - INFO - main.py - train - 87 - 【train】 epoch：99 step:199/200 loss：0.001432
2022-06-02 00:44:00,541 - INFO - main.py - train - 93 - 【dev】 loss：3.099472 accuracy：0.3636 micro_f1：0.3636 macro_f1：0.4325
2022-06-02 00:44:01,213 - INFO - main.py - <module> - 253 - ======== Carry Out Testing========
2022-06-02 00:45:17,446 - INFO - main.py - <module> - 257 - 【test】 loss：2.787870 accuracy：0.3636 micro_f1：0.3636 macro_f1：0.4325
2022-06-02 00:45:17,775 - INFO - main.py - <module> - 259 -
                        precision    recall  f1-score   support

            Negative       0.00      0.00      0.00         3
    Cause_of_disease       0.40      0.50      0.44         4
Treatment_of_disease       0.25      0.33      0.29         3
         Association       1.00      1.00      1.00         1

            accuracy                           0.36        11
           macro avg       0.41      0.46      0.43        11
        weighted avg       0.30      0.36      0.33        11

2022-06-02 00:45:17,853 - INFO - main.py - <module> - 263 - ======== Prediction ========
2022-06-02 00:45:33,249 - INFO - main.py - <module> - 275 - Halothane is known to oppose <e1start> digitalis <e1end>-induced <e2start> ventricular arrhythmias <e2end>. 
2022-06-02 00:45:35,858 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:45:35,874 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:45:35,874 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:35,874 - INFO - main.py - <module> - 275 - Both cases proved to be <e1start> cotton <e1end>-material-induced <e2start> granulomas <e2end>. 
2022-06-02 00:45:38,171 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:45:38,171 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:45:38,186 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:38,186 - INFO - main.py - <module> - 275 - The evidence for <e1start> soybean <e1end> products as <e2start> cancer <e2end> preventive agents.  
2022-06-02 00:45:40,342 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:45:40,342 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:45:40,342 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:40,342 - INFO - main.py - <module> - 275 - [Mortality trends in <e2start> cancer <e2end> attributable to <e1start> tobacco <e1end> in Mexico].  
2022-06-02 00:45:42,749 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:45:42,749 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:45:42,749 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:42,749 - INFO - main.py - <module> - 275 - <e1start> Areca <e1end> nut chewing has a significant association with <e2start> systemic inflammation <e2end>.
2022-06-02 00:45:45,038 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:45:45,038 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:45:45,038 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:45,054 - INFO - main.py - <module> - 275 - <e2start> major depression <e2end> (MD) and regular <e1start> tobacco <e1end> use (RU) or nicotine dependence (ND).
2022-06-02 00:45:47,225 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:45:47,225 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:45:47,225 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:47,225 - INFO - main.py - <module> - 275 - Benefits of whole <e1start> ginger <e1end> extract in <e2start> prostate cancer <e2end>.  
2022-06-02 00:45:49,507 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:45:49,507 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:45:49,507 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:49,507 - INFO - main.py - <module> - 275 - <e2start> schizophrenia <e2end> and <e1start> cannabis <e1end> is due to a shared genetic aetiology.
2022-06-02 00:45:51,663 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:45:51,663 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:45:51,663 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:51,663 - INFO - main.py - <module> - 275 - These risks may be even higher among <e1start> tobacco <e1end>-related <e2start> cancer <e2end> survivors (TRCS). 
2022-06-02 00:45:53,850 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:45:53,850 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:45:53,850 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:53,850 - INFO - main.py - <module> - 275 - Role of Dau c 1 in three different patterns of <e1start> carrot <e1end>-induced <e2start> asthma <e2end>.  
2022-06-02 00:45:55,944 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:45:55,944 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:45:55,944 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:45:55,944 - INFO - main.py - <module> - 275 - <e1start> Tobacco <e1end>-associated <e2start> cancers <e2end> included lung, esophageal, and H/N cancers. 
2022-06-02 00:46:06,085 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:06,085 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:06,085 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:06,085 - INFO - main.py - <module> - 275 - <e1start> Coffee <e1end> consumption not associated with risk of <e2start> pancreas cancer <e2end> in Finland.  
2022-06-02 00:46:08,413 - INFO - main.py - <module> - 277 - predict labels：Negative
2022-06-02 00:46:08,413 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:46:08,413 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:08,413 - INFO - main.py - <module> - 275 - Optimal dose of <e1start> garlic <e1end> to inhibit dimethylhydrazine-induced <e2start> colon cancer <e2end>.  
2022-06-02 00:46:10,460 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:10,460 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:46:10,460 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:10,460 - INFO - main.py - <module> - 275 - <e2start> major depression <e2end> (MD) and regular tobacco use (<e1start> RU <e1end>) or nicotine dependence (ND).
2022-06-02 00:46:12,585 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:12,585 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:46:12,585 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:12,585 - INFO - main.py - <module> - 275 - Chinese <e1start> green tea <e1end> ameliorates <e2start> lung injury <e2end> in cigarette smoke-exposed rats.  
2022-06-02 00:46:14,694 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:14,694 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:46:14,694 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:14,694 - INFO - main.py - <module> - 275 - Studies on magnesium's mechanism of action in <e1start> digitalis <e1end>-induced <e2start> arrhythmias <e2end>.  
2022-06-02 00:46:16,882 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:16,882 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:16,882 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:16,882 - INFO - main.py - <module> - 275 - Caffeine and <e1start> coffee <e1end> as therapeutics against <e2start> Alzheimer's disease <e2end>.  
2022-06-02 00:46:19,272 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:19,272 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:46:19,272 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:19,272 - INFO - main.py - <module> - 275 - recent <e1start> coffee <e1end> drinking and the incidence of <e2start> cardiovascular disease <e2end>.
2022-06-02 00:46:21,491 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:21,491 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:46:21,491 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:21,491 - INFO - main.py - <module> - 275 - In nonexposed, tobacco-, or <e1start> marijuana <e1end>-smoke-exposed <e2start> breast cancer <e2end> cultures. 
2022-06-02 00:46:23,897 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:23,897 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:46:23,897 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:23,897 - INFO - main.py - <module> - 275 - Aggressive <e1start> tobacco <e1end> control could avert millions of deaths from <e2start> tuberculosis <e2end>.  
2022-06-02 00:46:26,038 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:26,038 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:26,038 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:26,038 - INFO - main.py - <module> - 275 - She had <e2start> asthma <e2end> when handling raw <e1start> carrots <e1end>. 
2022-06-02 00:46:28,085 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:28,085 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:28,085 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:28,085 - INFO - main.py - <module> - 275 - <e1start> Tobacco <e1end>-associated cancers included lung, <e2start> esophageal, and H/N cancers <e2end>. 
2022-06-02 00:46:30,116 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:30,116 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:30,116 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:30,116 - INFO - main.py - <module> - 275 - major depression (<e2start> MD <e2end>) and regular <e1start> tobacco <e1end> use (RU) or nicotine dependence (ND).
2022-06-02 00:46:32,179 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:32,179 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:46:32,179 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:32,179 - INFO - main.py - <module> - 275 - Nasal congestion and coffee/<e1start> tea <e1end> intake also affected <e2start> xerostomia <e2end>. 
2022-06-02 00:46:34,241 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:34,241 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:46:34,241 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:34,241 - INFO - main.py - <module> - 275 - Association of <e1start> coffee <e1end> consumption with <e2start> gallbladder disease <e2end>.  
2022-06-02 00:46:36,288 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:36,288 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:46:36,288 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:36,288 - INFO - main.py - <module> - 275 - <e1start> coffee <e1end> consumption and the risk of <e2start> coronary heart disease <e2end> has been suspected.
2022-06-02 00:46:38,554 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:38,554 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:46:38,554 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:38,554 - INFO - main.py - <module> - 275 - Effects of smokeless <e1start> tobacco <e1end> use on other <e2start> cancers <e2end> are not clearly demonstrated. 
2022-06-02 00:46:40,725 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:40,725 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:46:40,725 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:40,725 - INFO - main.py - <module> - 275 - Therefore the minimal optimal dose of <e1start> garlic <e1end> to inhibit <e2start> colon cancer <e2end> was 2.5%. 
2022-06-02 00:46:42,913 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:42,913 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:46:42,913 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:42,913 - INFO - main.py - <module> - 275 - Consumption of <e1start> black tea <e1end> and <e2start> cancer <e2end> risk: a prospective cohort study.  
2022-06-02 00:46:44,897 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:44,897 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:46:44,897 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:44,897 - INFO - main.py - <module> - 275 - <e1start> Tobacco <e1end> smoke-induced <e2start> lung cancer <e2end> in animals--a challenge to toxicology (?).  
2022-06-02 00:46:47,007 - INFO - main.py - <module> - 277 - predict labels：Negative
2022-06-02 00:46:47,007 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:47,007 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:47,007 - INFO - main.py - <module> - 275 - She had <e2start> asthma <e2end> due to raw <e1start> carrot <e1end> ingestion and inhalation. 
2022-06-02 00:46:49,140 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:49,140 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:49,140 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:49,140 - INFO - main.py - <module> - 275 - <e1start> Green tea <e1end> also decreased the number of <e2start> dysplasic lesions <e2end>. 
2022-06-02 00:46:51,202 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:51,202 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:46:51,202 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:51,202 - INFO - main.py - <module> - 275 - <e1start> Coffee <e1end> consumption and the risk of <e2start> gestational diabetes mellitus <e2end>.  
2022-06-02 00:46:53,468 - INFO - main.py - <module> - 277 - predict labels：Negative
2022-06-02 00:46:53,468 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:46:53,468 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:53,468 - INFO - main.py - <module> - 275 - <e1start> Tobacco <e1end>-associated cancers included <e2start> lung, esophageal, and H/N cancers <e2end>. 
2022-06-02 00:46:55,468 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:55,468 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:55,468 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:55,468 - INFO - main.py - <module> - 275 - a relative risk estimate for RC <e1start> garlic <e1end> consumption and <e2start> cancer <e2end> risk.
2022-06-02 00:46:57,640 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:46:57,640 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:46:57,640 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:57,640 - INFO - main.py - <module> - 275 - MATERIAL AND METHODS: Patient 1 had <e2start> asthma <e2end> when handling raw <e1start> carrots <e1end>. 
2022-06-02 00:46:59,858 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:46:59,858 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:46:59,858 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:46:59,858 - INFO - main.py - <module> - 275 - Nasal congestion and <e1start> coffee <e1end>/tea intake also affected <e2start> xerostomia <e2end>. 
2022-06-02 00:47:02,186 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:47:02,186 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:47:02,186 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:02,186 - INFO - main.py - <module> - 275 - <e1start> Coffee <e1end>, caffeine, and <e2start> cardiovascular disease <e2end> in men.  
2022-06-02 00:47:04,296 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:47:04,296 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:47:04,296 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:04,296 - INFO - main.py - <module> - 275 - <e1start> Green tea <e1end> and death from <e2start> pneumonia <e2end> in Japan: the Ohsaki cohort study.  
2022-06-02 00:47:06,405 - INFO - main.py - <module> - 277 - predict labels：Negative
2022-06-02 00:47:06,405 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:47:06,405 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:06,405 - INFO - main.py - <module> - 275 - The efficacy of <e1start> Ginkgo biloba <e1end> on cognitive function in <e2start> Alzheimer disease <e2end>.  
2022-06-02 00:47:08,358 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:47:08,374 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:47:08,374 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:08,374 - INFO - main.py - <module> - 275 - <e2start> Atrial tachyarrhythmias <e2end> are a common manifestation of <e1start> digitalis <e1end> toxicity. 
2022-06-02 00:47:10,343 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:47:10,343 - INFO - main.py - <module> - 278 - true label：Cause_of_disease
2022-06-02 00:47:10,343 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:10,343 - INFO - main.py - <module> - 275 - major depression (<e2start> MD <e2end>) and regular tobacco use (<e1start> RU <e1end>) or nicotine dependence (ND).
2022-06-02 00:47:12,686 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:47:12,686 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:47:12,686 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:12,686 - INFO - main.py - <module> - 275 - In nonexposed, <e1start> tobacco <e1end>-, or marijuana-smoke-exposed <e2start> breast cancer <e2end> cultures. 
2022-06-02 00:47:14,858 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:47:14,858 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:47:14,858 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:14,858 - INFO - main.py - <module> - 275 - suggestive of population differences in how <e2start> MD <e2end> and <e1start> tobacco <e1end> use inter-relate.
2022-06-02 00:47:16,968 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:47:16,968 - INFO - main.py - <module> - 278 - true label：Association
2022-06-02 00:47:16,968 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:16,968 - INFO - main.py - <module> - 275 - CONCLUSION: <e1start> Lespedeza Michx <e1end> was effective on <e2start> MCN <e2end>.  
2022-06-02 00:47:18,983 - INFO - main.py - <module> - 277 - predict labels：Cause_of_disease
2022-06-02 00:47:18,983 - INFO - main.py - <module> - 278 - true label：Treatment_of_disease
2022-06-02 00:47:18,983 - INFO - main.py - <module> - 279 - ==========================
2022-06-02 00:47:18,983 - INFO - main.py - <module> - 275 - <e1start> Coffee <e1end> consumption and risk of <e2start> rheumatoid arthritis <e2end>.  
2022-06-02 00:47:21,015 - INFO - main.py - <module> - 277 - predict labels：Association
2022-06-02 00:47:21,015 - INFO - main.py - <module> - 278 - true label：Negative
2022-06-02 00:47:21,015 - INFO - main.py - <module> - 279 - ==========================
